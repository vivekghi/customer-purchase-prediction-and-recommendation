{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Sagemaker built in Algo - Factorization Machine we will build the recommender system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use the data prepared in previous script as starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:00:00.070561Z",
     "iopub.status.busy": "2021-11-16T12:00:00.070128Z",
     "iopub.status.idle": "2021-11-16T12:00:00.775776Z",
     "shell.execute_reply": "2021-11-16T12:00:00.774685Z",
     "shell.execute_reply.started": "2021-11-16T12:00:00.070502Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "#import sagemaker\n",
    "#import sagemaker.amazon.common as smac\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack, save_npz, load_npz\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('max_colwidth', 50)  # default is 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:00:00.781449Z",
     "iopub.status.busy": "2021-11-16T12:00:00.780992Z",
     "iopub.status.idle": "2021-11-16T12:00:00.787939Z",
     "shell.execute_reply": "2021-11-16T12:00:00.786970Z",
     "shell.execute_reply.started": "2021-11-16T12:00:00.781405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.20.3\n",
      "pandas version: 1.0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"numpy version:\", np.__version__)\n",
    "print(\"pandas version:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:00:00.790291Z",
     "iopub.status.busy": "2021-11-16T12:00:00.789859Z",
     "iopub.status.idle": "2021-11-16T12:00:03.984668Z",
     "shell.execute_reply": "2021-11-16T12:00:03.983575Z",
     "shell.execute_reply.started": "2021-11-16T12:00:00.790249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 3347127 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>product_group_name</th>\n",
       "      <th>detail_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2685810</th>\n",
       "      <td>2685810</td>\n",
       "      <td>734127</td>\n",
       "      <td>842062003</td>\n",
       "      <td>1</td>\n",
       "      <td>Garment Lower body</td>\n",
       "      <td>High-waisted, calf-length skirt in an airy vis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833504</th>\n",
       "      <td>833504</td>\n",
       "      <td>158537</td>\n",
       "      <td>854619003</td>\n",
       "      <td>1</td>\n",
       "      <td>Swimwear</td>\n",
       "      <td>Fully lined bikini bottoms. Mid waist with wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2049984</th>\n",
       "      <td>2049984</td>\n",
       "      <td>483185</td>\n",
       "      <td>372860001</td>\n",
       "      <td>1</td>\n",
       "      <td>Socks &amp; Tights</td>\n",
       "      <td>Fine-knit trainer socks in a soft cotton blend.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358900</th>\n",
       "      <td>358900</td>\n",
       "      <td>64893</td>\n",
       "      <td>832458002</td>\n",
       "      <td>1</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>Fitted top in stretch, ribbed jersey with a wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298175</th>\n",
       "      <td>298175</td>\n",
       "      <td>53793</td>\n",
       "      <td>839496002</td>\n",
       "      <td>1</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>Fitted top in jersey with a low-cut back and l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           index  customer_id  article_id  rating  product_group_name  \\\n",
       "2685810  2685810       734127   842062003       1  Garment Lower body   \n",
       "833504    833504       158537   854619003       1            Swimwear   \n",
       "2049984  2049984       483185   372860001       1      Socks & Tights   \n",
       "358900    358900        64893   832458002       1  Garment Upper body   \n",
       "298175    298175        53793   839496002       1  Garment Upper body   \n",
       "\n",
       "                                               detail_desc  \n",
       "2685810  High-waisted, calf-length skirt in an airy vis...  \n",
       "833504   Fully lined bikini bottoms. Mid waist with wid...  \n",
       "2049984    Fine-knit trainer socks in a soft cotton blend.  \n",
       "358900   Fitted top in stretch, ribbed jersey with a wi...  \n",
       "298175   Fitted top in jersey with a low-cut back and l...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set \"usecols\" to prevent the following error:\n",
    "# ParserError: Error tokenizing data. C error: Expected 15 fields in line 1598, saw 22\n",
    "\n",
    "df_rank = pd.read_csv(\"fm_preprocessed_filtered_with_attributes.csv\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Total records:\", df_rank.shape[0], \"\\n\")\n",
    "#print(\"Sample records:\\n\")\n",
    "df_rank.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rank[\"detail_desc\"].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:00:05.943221Z",
     "iopub.status.busy": "2021-11-16T12:00:05.942266Z",
     "iopub.status.idle": "2021-11-16T12:00:06.211860Z",
     "shell.execute_reply": "2021-11-16T12:00:06.210727Z",
     "shell.execute_reply.started": "2021-11-16T12:00:05.943168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique customers: 680578\n",
      "Unique products: 1684\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique customers:\", df_rank[\"customer_id\"].nunique())\n",
    "print(\"Unique products:\", df_rank[\"article_id\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the sparse matrix\n",
    "#### For categorical columns use OneHotEncoder\n",
    "#### For text columns use TfIdf for text embeddings from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3347127x682272 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10041381 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = OneHotEncoder(handle_unknown = \"ignore\")\n",
    "ohe_cols = [\"customer_id\", \"article_id\",\"product_group_name\"]\n",
    "ohe_features = ohe.fit_transform(df_rank[ohe_cols])\n",
    "ohe_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rank.detail_desc.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3347127x482 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 65490369 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2)  \n",
    "vectorizer.fit(df_rank[\"detail_desc\"].unique())\n",
    "tfidf_features = vectorizer.transform(df_rank[\"detail_desc\"])\n",
    "tfidf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3347127x682754 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 75531750 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = hstack([ohe_features,tfidf_features], format=\"csr\", dtype=\"float32\")\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:00:06.365078Z",
     "iopub.status.busy": "2021-11-16T12:00:06.364169Z",
     "iopub.status.idle": "2021-11-16T12:00:06.538229Z",
     "shell.execute_reply": "2021-11-16T12:00:06.537281Z",
     "shell.execute_reply.started": "2021-11-16T12:00:06.365026Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 5., 5., 5.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df_rank[\"rating\"].values.astype(\"float32\")\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:00:06.540439Z",
     "iopub.status.busy": "2021-11-16T12:00:06.539823Z",
     "iopub.status.idle": "2021-11-16T12:00:06.561893Z",
     "shell.execute_reply": "2021-11-16T12:00:06.560938Z",
     "shell.execute_reply.started": "2021-11-16T12:00:06.540396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elements: 2285264347758\n",
      "Non-zero elements: 75531750\n",
      "Sparsity: 99.9967 %\n"
     ]
    }
   ],
   "source": [
    "total = X.shape[0] * X.shape[1]\n",
    "non_zero = X.nnz\n",
    "sparsity = (total - non_zero) / total\n",
    "\n",
    "print(\"Total elements:\", total)\n",
    "print(\"Non-zero elements:\", non_zero)\n",
    "print(\"Sparsity:\", round(sparsity*100, 4), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spilt the data in training and testing set. The rating is the prediction label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:00:06.563421Z",
     "iopub.status.busy": "2021-11-16T12:00:06.563153Z",
     "iopub.status.idle": "2021-11-16T12:00:06.621047Z",
     "shell.execute_reply": "2021-11-16T12:00:06.620351Z",
     "shell.execute_reply.started": "2021-11-16T12:00:06.563392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (2677701, 682754)\n",
      "Shape of y_train: (2677701,) \n",
      "\n",
      "Shape of X_test: (669426, 682754)\n",
      "Shape of y_test: (669426,)\n"
     ]
    }
   ],
   "source": [
    "# By default, shuffle=True.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=73)           \n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape, \"\\n\")\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:00:06.625973Z",
     "iopub.status.busy": "2021-11-16T12:00:06.625446Z",
     "iopub.status.idle": "2021-11-16T12:00:06.665598Z",
     "shell.execute_reply": "2021-11-16T12:00:06.664599Z",
     "shell.execute_reply.started": "2021-11-16T12:00:06.625924Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_dim = X.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create sparse RecordIO file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def write_sparse_recordio_file (filename, X, y=None):\n",
    "    with open(filename, 'wb') as f:\n",
    "        smac.write_spmatrix_to_sparse_tensor (f, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upload file to S3.\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.upload_fileobj\n",
    "\n",
    "def upload_to_s3(filename, bucket, prefix, key):\n",
    "    with open(filename,'rb') as f: # Read in binary mode\n",
    "        boto3.Session().resource('s3').Bucket(bucket).Object(f\"{prefix}/{key}\").upload_fileobj(f)\n",
    "        return f\"s3://{bucket}/{prefix}/{key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_sparse_recordio_file(\"fm_train_filtered_sparse.recordio\", X_train, y_train)\n",
    "write_sparse_recordio_file(\"fm_test_filtered_sparse.recordio\", X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the train and test RecordIO files to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker version: 2.70.0\n",
      "Region: ap-south-1\n",
      "Bucket: sagemaker-ap-south-1-659144925604\n",
      "train file location: s3://sagemaker-ap-south-1-659144925604/filteredfm/fm_train_filtered_sparse.recordio\n",
      "test file location: s3://sagemaker-ap-south-1-659144925604/filteredfm/fm_test_filtered_sparse.recordio\n",
      "model output location: s3://sagemaker-ap-south-1-659144925604/filteredfm/output\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "prefix = \"filteredfm\"\n",
    "train_key = \"fm_train_filtered_sparse.recordio\"\n",
    "test_key = \"fm_test_filtered_sparse.recordio\"\n",
    "output_location = f\"s3://{bucket}/{prefix}/output\"\n",
    "\n",
    "train_file_location = upload_to_s3(\"fm_train_filtered_sparse.recordio\", bucket, prefix, train_key)\n",
    "test_file_location = upload_to_s3(\"fm_test_filtered_sparse.recordio\", bucket, prefix, test_key)\n",
    "\n",
    "print(\"SageMaker version:\", sagemaker.__version__)\n",
    "print(\"Region:\", region)\n",
    "print(\"Bucket:\", bucket)\n",
    "print(\"train file location:\", train_file_location)\n",
    "print(\"test file location:\", test_file_location)\n",
    "print(\"model output location:\", output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fm-job-recommender-v1'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_name = 'fm-job-recommender-v1'\n",
    "job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint uri: None\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/aws-samples/amazon-sagemaker-managed-spot-training/blob/main/xgboost_built_in_managed_spot_training_checkpointing/xgboost_built_in_managed_spot_training_checkpointing.ipynb\n",
    "    \n",
    "use_spot_instances = False\n",
    "max_run = 3600                                   # set to 60 mins\n",
    "max_wait = 3600 if use_spot_instances else None  # set to 60 mins (must be equal or greater than max_run)\n",
    "   \n",
    "checkpoint_s3_uri = (f\"s3://{bucket}/{prefix}/checkpoints/{job_name}\" if use_spot_instances\n",
    "                     else None)\n",
    "    \n",
    "print(f\"Checkpoint uri: {checkpoint_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'991648021394.dkr.ecr.ap-south-1.amazonaws.com/factorization-machines:1'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "container = sagemaker.image_uris.retrieve(\"factorization-machines\", region=region)\n",
    "container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define estimator as built in Factorization machine model in Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(    \n",
    "    container,\n",
    "    role,\n",
    "    instance_count = 1,\n",
    "    instance_type = \"ml.m4.xlarge\",   # Or \"ml.c5.xlarge\",\n",
    "    output_path = output_location,\n",
    "    sagemaker_session = sess,\n",
    "    base_job_name = job_name,\n",
    "    use_spot_instances = use_spot_instances,\n",
    "    max_run = max_run,\n",
    "    max_wait = max_wait,\n",
    "    checkpoint_s3_uri = checkpoint_s3_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "682754"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_dim': 682754,\n",
       " 'num_factors': 64,\n",
       " 'predictor_type': 'regressor',\n",
       " 'epochs': 5,\n",
       " 'mini_batch_size': 2000}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.set_hyperparameters(\n",
    "    feature_dim = feature_dim,\n",
    "    num_factors = 64,  \n",
    "    predictor_type = \"regressor\",\n",
    "    epochs = 5,      \n",
    "    mini_batch_size = 2000,  \n",
    ")\n",
    "\n",
    "estimator.hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start trainig job in Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-05 11:41:24 Starting - Starting the training job...\n",
      "2022-04-05 11:41:49 Starting - Preparing the instances for trainingProfilerReport-1649158884: InProgress\n",
      ".........\n",
      "2022-04-05 11:43:08 Downloading - Downloading input data...\n",
      "2022-04-05 11:43:48 Training - Downloading the training image...\n",
      "2022-04-05 11:44:25 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:87: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:120: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:44:30 INFO 140669154469696 integration.py:636] worker started\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:44:30 INFO 140669154469696] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-conf.json: {'epochs': 1, 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0'}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:44:30 INFO 140669154469696] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'epochs': '5', 'feature_dim': '682754', 'mini_batch_size': '2000', 'num_factors': '64', 'predictor_type': 'regressor'}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:44:30 INFO 140669154469696] Final configuration: {'epochs': '5', 'mini_batch_size': '2000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0', 'feature_dim': '682754', 'num_factors': '64', 'predictor_type': 'regressor'}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:44:30 WARNING 140669154469696] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:44:30 INFO 140669154469696] Using default worker.\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:44:30 INFO 140669154469696] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2022-04-05 11:44:30.254] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2022-04-05 11:44:30.276] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 43, \"num_examples\": 1, \"num_bytes\": 426944}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:44:30 INFO 140669154469696] nvidia-smi: took 0.033 seconds to run.\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:44:30 INFO 140669154469696] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:44:30 INFO 140669154469696] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:44:30 INFO 140669154469696] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:44:30 INFO 140669154469696] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1649159070.2322664, \"EndTime\": 1649159070.3161783, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 45.468807220458984, \"count\": 1, \"min\": 45.468807220458984, \"max\": 45.468807220458984}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1649159070.316364, \"EndTime\": 1649159070.3164234, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2000.0, \"count\": 1, \"min\": 2000, \"max\": 2000}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 2000.0, \"count\": 1, \"min\": 2000, \"max\": 2000}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[34m[11:44:30] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.206299.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[11:44:30] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.206299.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:44:33 INFO 140669154469696] #quality_metric: host=algo-1, epoch=0, batch=0 train rmse <loss>=1.26560059775858\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:44:33 INFO 140669154469696] #quality_metric: host=algo-1, epoch=0, batch=0 train mse <loss>=1.601744873046875\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:44:33 INFO 140669154469696] #quality_metric: host=algo-1, epoch=0, batch=0 train absolute_loss <loss>=1.176975830078125\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:44:57 INFO 140669154469696] Iter[0] Batch [500]#011Speed: 42271.23 samples/sec\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:44:57 INFO 140669154469696] #quality_metric: host=algo-1, epoch=0, batch=500 train rmse <loss>=0.46273514588340864\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:44:57 INFO 140669154469696] #quality_metric: host=algo-1, epoch=0, batch=500 train mse <loss>=0.21412381523573945\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:44:57 INFO 140669154469696] #quality_metric: host=algo-1, epoch=0, batch=500 train absolute_loss <loss>=0.30245858481401455\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:45:20 INFO 140669154469696] Iter[0] Batch [1000]#011Speed: 41903.91 samples/sec\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:45:20 INFO 140669154469696] #quality_metric: host=algo-1, epoch=0, batch=1000 train rmse <loss>=0.45347360960922933\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:45:20 INFO 140669154469696] #quality_metric: host=algo-1, epoch=0, batch=1000 train mse <loss>=0.20563831461202373\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:45:20 INFO 140669154469696] #quality_metric: host=algo-1, epoch=0, batch=1000 train absolute_loss <loss>=0.294761874310382\u001b[0m\n",
      "\u001b[34m[2022-04-05 11:45:36.474] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 64059, \"num_examples\": 1339, \"num_bytes\": 569604428}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:45:36 INFO 140669154469696] #quality_metric: host=algo-1, epoch=0, train rmse <loss>=0.45043718740915484\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:45:36 INFO 140669154469696] #quality_metric: host=algo-1, epoch=0, train mse <loss>=0.20289365980107008\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:45:36 INFO 140669154469696] #quality_metric: host=algo-1, epoch=0, train absolute_loss <loss>=0.29267530590282437\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1649159070.3162937, \"EndTime\": 1649159136.4752872, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"update.time\": {\"sum\": 66158.48302841187, \"count\": 1, \"min\": 66158.48302841187, \"max\": 66158.48302841187}}}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:45:36 INFO 140669154469696] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1649159070.3167567, \"EndTime\": 1649159136.4755917, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2679701.0, \"count\": 1, \"min\": 2679701, \"max\": 2679701}, \"Total Batches Seen\": {\"sum\": 1340.0, \"count\": 1, \"min\": 1340, \"max\": 1340}, \"Max Records Seen Between Resets\": {\"sum\": 2677701.0, \"count\": 1, \"min\": 2677701, \"max\": 2677701}, \"Max Batches Seen Between Resets\": {\"sum\": 1339.0, \"count\": 1, \"min\": 1339, \"max\": 1339}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 2677701.0, \"count\": 1, \"min\": 2677701, \"max\": 2677701}, \"Number of Batches Since Last Reset\": {\"sum\": 1339.0, \"count\": 1, \"min\": 1339, \"max\": 1339}}}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:45:36 INFO 140669154469696] #throughput_metric: host=algo-1, train throughput=40473.73221740817 records/second\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:45:36 INFO 140669154469696] #quality_metric: host=algo-1, epoch=1, batch=0 train rmse <loss>=0.4548590037108598\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:45:36 INFO 140669154469696] #quality_metric: host=algo-1, epoch=1, batch=0 train mse <loss>=0.20689671325683595\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:45:36 INFO 140669154469696] #quality_metric: host=algo-1, epoch=1, batch=0 train absolute_loss <loss>=0.2856169738769531\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:45:59 INFO 140669154469696] Iter[1] Batch [500]#011Speed: 43218.01 samples/sec\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:45:59 INFO 140669154469696] #quality_metric: host=algo-1, epoch=1, batch=500 train rmse <loss>=0.4341615542989001\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:45:59 INFO 140669154469696] #quality_metric: host=algo-1, epoch=1, batch=500 train mse <loss>=0.18849625523123675\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:45:59 INFO 140669154469696] #quality_metric: host=algo-1, epoch=1, batch=500 train absolute_loss <loss>=0.28199680701463287\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:46:23 INFO 140669154469696] Iter[1] Batch [1000]#011Speed: 42174.82 samples/sec\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:46:23 INFO 140669154469696] #quality_metric: host=algo-1, epoch=1, batch=1000 train rmse <loss>=0.43156009586386923\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:46:23 INFO 140669154469696] #quality_metric: host=algo-1, epoch=1, batch=1000 train mse <loss>=0.186244116342032\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:46:23 INFO 140669154469696] #quality_metric: host=algo-1, epoch=1, batch=1000 train absolute_loss <loss>=0.28011929734389146\u001b[0m\n",
      "\u001b[34m[2022-04-05 11:46:39.045] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 62564, \"num_examples\": 1339, \"num_bytes\": 569604428}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:46:39 INFO 140669154469696] #quality_metric: host=algo-1, epoch=1, train rmse <loss>=0.4300912984911638\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:46:39 INFO 140669154469696] #quality_metric: host=algo-1, epoch=1, train mse <loss>=0.18497852503781537\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:46:39 INFO 140669154469696] #quality_metric: host=algo-1, epoch=1, train absolute_loss <loss>=0.2791116533571433\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1649159136.4753833, \"EndTime\": 1649159199.0457268, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 62569.78487968445, \"count\": 1, \"min\": 62569.78487968445, \"max\": 62569.78487968445}}}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:46:39 INFO 140669154469696] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1649159136.4759052, \"EndTime\": 1649159199.0459666, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5357402.0, \"count\": 1, \"min\": 5357402, \"max\": 5357402}, \"Total Batches Seen\": {\"sum\": 2679.0, \"count\": 1, \"min\": 2679, \"max\": 2679}, \"Max Records Seen Between Resets\": {\"sum\": 2677701.0, \"count\": 1, \"min\": 2677701, \"max\": 2677701}, \"Max Batches Seen Between Resets\": {\"sum\": 1339.0, \"count\": 1, \"min\": 1339, \"max\": 1339}, \"Reset Count\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Number of Records Since Last Reset\": {\"sum\": 2677701.0, \"count\": 1, \"min\": 2677701, \"max\": 2677701}, \"Number of Batches Since Last Reset\": {\"sum\": 1339.0, \"count\": 1, \"min\": 1339, \"max\": 1339}}}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:46:39 INFO 140669154469696] #throughput_metric: host=algo-1, train throughput=42795.14150028066 records/second\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:46:39 INFO 140669154469696] #quality_metric: host=algo-1, epoch=2, batch=0 train rmse <loss>=0.44066071926918554\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:46:39 INFO 140669154469696] #quality_metric: host=algo-1, epoch=2, batch=0 train mse <loss>=0.19418186950683594\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:46:39 INFO 140669154469696] #quality_metric: host=algo-1, epoch=2, batch=0 train absolute_loss <loss>=0.2769175109863281\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:47:02 INFO 140669154469696] Iter[2] Batch [500]#011Speed: 42763.72 samples/sec\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:47:02 INFO 140669154469696] #quality_metric: host=algo-1, epoch=2, batch=500 train rmse <loss>=0.4189200615583012\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:47:02 INFO 140669154469696] #quality_metric: host=algo-1, epoch=2, batch=500 train mse <loss>=0.17549401797601086\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:47:02 INFO 140669154469696] #quality_metric: host=algo-1, epoch=2, batch=500 train absolute_loss <loss>=0.27133850298670237\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:47:26 INFO 140669154469696] Iter[2] Batch [1000]#011Speed: 42165.32 samples/sec\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:47:26 INFO 140669154469696] #quality_metric: host=algo-1, epoch=2, batch=1000 train rmse <loss>=0.41658229516884054\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:47:26 INFO 140669154469696] #quality_metric: host=algo-1, epoch=2, batch=1000 train mse <loss>=0.17354080864813898\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:47:26 INFO 140669154469696] #quality_metric: host=algo-1, epoch=2, batch=1000 train absolute_loss <loss>=0.2692855138788214\u001b[0m\n",
      "\u001b[34m[2022-04-05 11:47:41.938] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 62875, \"num_examples\": 1339, \"num_bytes\": 569604428}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:47:41 INFO 140669154469696] #quality_metric: host=algo-1, epoch=2, train rmse <loss>=0.4153233791120465\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:47:41 INFO 140669154469696] #quality_metric: host=algo-1, epoch=2, train mse <loss>=0.1724935092370487\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:47:41 INFO 140669154469696] #quality_metric: host=algo-1, epoch=2, train absolute_loss <loss>=0.2682324333390342\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1649159199.0458064, \"EndTime\": 1649159261.9390335, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 62879.03928756714, \"count\": 1, \"min\": 62879.03928756714, \"max\": 62879.03928756714}}}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:47:41 INFO 140669154469696] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1649159199.059939, \"EndTime\": 1649159261.9393723, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 8035103.0, \"count\": 1, \"min\": 8035103, \"max\": 8035103}, \"Total Batches Seen\": {\"sum\": 4018.0, \"count\": 1, \"min\": 4018, \"max\": 4018}, \"Max Records Seen Between Resets\": {\"sum\": 2677701.0, \"count\": 1, \"min\": 2677701, \"max\": 2677701}, \"Max Batches Seen Between Resets\": {\"sum\": 1339.0, \"count\": 1, \"min\": 1339, \"max\": 1339}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 2677701.0, \"count\": 1, \"min\": 2677701, \"max\": 2677701}, \"Number of Batches Since Last Reset\": {\"sum\": 1339.0, \"count\": 1, \"min\": 1339, \"max\": 1339}}}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:47:41 INFO 140669154469696] #throughput_metric: host=algo-1, train throughput=42584.57090600376 records/second\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:47:42 INFO 140669154469696] #quality_metric: host=algo-1, epoch=3, batch=0 train rmse <loss>=0.4260689072733625\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:47:42 INFO 140669154469696] #quality_metric: host=algo-1, epoch=3, batch=0 train mse <loss>=0.18153471374511718\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:47:42 INFO 140669154469696] #quality_metric: host=algo-1, epoch=3, batch=0 train absolute_loss <loss>=0.27712191772460937\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:48:05 INFO 140669154469696] Iter[3] Batch [500]#011Speed: 42109.33 samples/sec\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:48:05 INFO 140669154469696] #quality_metric: host=algo-1, epoch=3, batch=500 train rmse <loss>=0.40610523480191113\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:48:05 INFO 140669154469696] #quality_metric: host=algo-1, epoch=3, batch=500 train mse <loss>=0.16492146173351538\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:48:05 INFO 140669154469696] #quality_metric: host=algo-1, epoch=3, batch=500 train absolute_loss <loss>=0.2611327412314044\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:48:28 INFO 140669154469696] Iter[3] Batch [1000]#011Speed: 43083.75 samples/sec\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:48:28 INFO 140669154469696] #quality_metric: host=algo-1, epoch=3, batch=1000 train rmse <loss>=0.4041326915202643\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:48:28 INFO 140669154469696] #quality_metric: host=algo-1, epoch=3, batch=1000 train mse <loss>=0.16332323235541313\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:48:28 INFO 140669154469696] #quality_metric: host=algo-1, epoch=3, batch=1000 train absolute_loss <loss>=0.2592572573758029\u001b[0m\n",
      "\u001b[34m[2022-04-05 11:48:44.449] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 62505, \"num_examples\": 1339, \"num_bytes\": 569604428}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:48:44 INFO 140669154469696] #quality_metric: host=algo-1, epoch=3, train rmse <loss>=0.40309625286552075\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:48:44 INFO 140669154469696] #quality_metric: host=algo-1, epoch=3, train mse <loss>=0.16248658907422386\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:48:44 INFO 140669154469696] #quality_metric: host=algo-1, epoch=3, train absolute_loss <loss>=0.25832005142542386\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1649159261.9391441, \"EndTime\": 1649159324.449936, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 62508.60905647278, \"count\": 1, \"min\": 62508.60905647278, \"max\": 62508.60905647278}}}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:48:44 INFO 140669154469696] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1649159261.9412916, \"EndTime\": 1649159324.450247, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 10712804.0, \"count\": 1, \"min\": 10712804, \"max\": 10712804}, \"Total Batches Seen\": {\"sum\": 5357.0, \"count\": 1, \"min\": 5357, \"max\": 5357}, \"Max Records Seen Between Resets\": {\"sum\": 2677701.0, \"count\": 1, \"min\": 2677701, \"max\": 2677701}, \"Max Batches Seen Between Resets\": {\"sum\": 1339.0, \"count\": 1, \"min\": 1339, \"max\": 1339}, \"Reset Count\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Number of Records Since Last Reset\": {\"sum\": 2677701.0, \"count\": 1, \"min\": 2677701, \"max\": 2677701}, \"Number of Batches Since Last Reset\": {\"sum\": 1339.0, \"count\": 1, \"min\": 1339, \"max\": 1339}}}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:48:44 INFO 140669154469696] #throughput_metric: host=algo-1, train throughput=42836.967035856556 records/second\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:48:44 INFO 140669154469696] #quality_metric: host=algo-1, epoch=4, batch=0 train rmse <loss>=0.41393685446263645\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:48:44 INFO 140669154469696] #quality_metric: host=algo-1, epoch=4, batch=0 train mse <loss>=0.17134371948242189\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:48:44 INFO 140669154469696] #quality_metric: host=algo-1, epoch=4, batch=0 train absolute_loss <loss>=0.2728624572753906\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:49:08 INFO 140669154469696] Iter[4] Batch [500]#011Speed: 42071.64 samples/sec\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:49:08 INFO 140669154469696] #quality_metric: host=algo-1, epoch=4, batch=500 train rmse <loss>=0.3956470557496501\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:49:08 INFO 140669154469696] #quality_metric: host=algo-1, epoch=4, batch=500 train mse <loss>=0.15653659272336673\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:49:08 INFO 140669154469696] #quality_metric: host=algo-1, epoch=4, batch=500 train absolute_loss <loss>=0.2523225039788587\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:49:31 INFO 140669154469696] Iter[4] Batch [1000]#011Speed: 43391.84 samples/sec\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:49:31 INFO 140669154469696] #quality_metric: host=algo-1, epoch=4, batch=1000 train rmse <loss>=0.3939683440842865\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:49:31 INFO 140669154469696] #quality_metric: host=algo-1, epoch=4, batch=1000 train mse <loss>=0.15521105614051475\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:49:31 INFO 140669154469696] #quality_metric: host=algo-1, epoch=4, batch=1000 train absolute_loss <loss>=0.25063556960007766\u001b[0m\n",
      "\u001b[34m[2022-04-05 11:49:46.980] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 62515, \"num_examples\": 1339, \"num_bytes\": 569604428}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:49:46 INFO 140669154469696] #quality_metric: host=algo-1, epoch=4, train rmse <loss>=0.3930882825709903\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:49:46 INFO 140669154469696] #quality_metric: host=algo-1, epoch=4, train mse <loss>=0.15451839789461075\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:49:46 INFO 140669154469696] #quality_metric: host=algo-1, epoch=4, train absolute_loss <loss>=0.24981371021911758\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:49:46 INFO 140669154469696] #quality_metric: host=algo-1, train rmse <loss>=0.3930882825709903\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:49:46 INFO 140669154469696] #quality_metric: host=algo-1, train mse <loss>=0.15451839789461075\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:49:46 INFO 140669154469696] #quality_metric: host=algo-1, train absolute_loss <loss>=0.24981371021911758\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1649159324.4500363, \"EndTime\": 1649159386.9818292, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 62519.261837005615, \"count\": 1, \"min\": 62519.261837005615, \"max\": 62519.261837005615}}}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:49:46 INFO 140669154469696] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1649159324.4625394, \"EndTime\": 1649159386.9820223, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 13390505.0, \"count\": 1, \"min\": 13390505, \"max\": 13390505}, \"Total Batches Seen\": {\"sum\": 6696.0, \"count\": 1, \"min\": 6696, \"max\": 6696}, \"Max Records Seen Between Resets\": {\"sum\": 2677701.0, \"count\": 1, \"min\": 2677701, \"max\": 2677701}, \"Max Batches Seen Between Resets\": {\"sum\": 1339.0, \"count\": 1, \"min\": 1339, \"max\": 1339}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 2677701.0, \"count\": 1, \"min\": 2677701, \"max\": 2677701}, \"Number of Batches Since Last Reset\": {\"sum\": 1339.0, \"count\": 1, \"min\": 1339, \"max\": 1339}}}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:49:46 INFO 140669154469696] #throughput_metric: host=algo-1, train throughput=42829.767488299316 records/second\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:49:46 WARNING 140669154469696] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:49:46 INFO 140669154469696] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1649159386.9818919, \"EndTime\": 1649159387.1719704, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 189.55373764038086, \"count\": 1, \"min\": 189.55373764038086, \"max\": 189.55373764038086}}}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:49:47 INFO 140669154469696] Saved checkpoint to \"/tmp/tmpq86d3ihz/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2022-04-05 11:49:48.561] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 318307, \"num_examples\": 1, \"num_bytes\": 426588}\u001b[0m\n",
      "\u001b[34m[2022-04-05 11:50:41.814] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 53252, \"num_examples\": 335, \"num_bytes\": 142361188}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1649159388.5611498, \"EndTime\": 1649159441.8147619, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 669426.0, \"count\": 1, \"min\": 669426, \"max\": 669426}, \"Total Batches Seen\": {\"sum\": 335.0, \"count\": 1, \"min\": 335, \"max\": 335}, \"Max Records Seen Between Resets\": {\"sum\": 669426.0, \"count\": 1, \"min\": 669426, \"max\": 669426}, \"Max Batches Seen Between Resets\": {\"sum\": 335.0, \"count\": 1, \"min\": 335, \"max\": 335}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 669426.0, \"count\": 1, \"min\": 669426, \"max\": 669426}, \"Number of Batches Since Last Reset\": {\"sum\": 335.0, \"count\": 1, \"min\": 335, \"max\": 335}}}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:50:41 INFO 140669154469696] #test_score (algo-1) : ('rmse', 0.42768756940760405)\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:50:41 INFO 140669154469696] #test_score (algo-1) : ('mse', 0.18291665702578414)\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:50:41 INFO 140669154469696] #test_score (algo-1) : ('absolute_loss', 0.2817759122840903)\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:50:41 INFO 140669154469696] #quality_metric: host=algo-1, test rmse <loss>=0.42768756940760405\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:50:41 INFO 140669154469696] #quality_metric: host=algo-1, test mse <loss>=0.18291665702578414\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:50:41 INFO 140669154469696] #quality_metric: host=algo-1, test absolute_loss <loss>=0.2817759122840903\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1649159387.1721616, \"EndTime\": 1649159441.816023, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 29.193878173828125, \"count\": 1, \"min\": 29.193878173828125, \"max\": 29.193878173828125}, \"totaltime\": {\"sum\": 371621.83356285095, \"count\": 1, \"min\": 371621.83356285095, \"max\": 371621.83356285095}}}\u001b[0m\n",
      "\u001b[34m[04/05/2022 11:50:41 INFO 140669154469696 integration.py:636] worker closed\u001b[0m\n",
      "\n",
      "2022-04-05 11:50:49 Uploading - Uploading generated training model\n",
      "2022-04-05 11:51:30 Completed - Training job completed\n",
      "ProfilerReport-1649158884: NoIssuesFound\n",
      "Training seconds: 499\n",
      "Billable seconds: 499\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'train':train_file_location, \n",
    "               'test':test_file_location})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TrainingJobName': 'fm-job-recommender-v1-2022-04-05-11-41-24-492',\n",
       " 'TrainingJobArn': 'arn:aws:sagemaker:ap-south-1:659144925604:training-job/fm-job-recommender-v1-2022-04-05-11-41-24-492',\n",
       " 'ModelArtifacts': {'S3ModelArtifacts': 's3://sagemaker-ap-south-1-659144925604/filteredfm/output/fm-job-recommender-v1-2022-04-05-11-41-24-492/output/model.tar.gz'},\n",
       " 'TrainingJobStatus': 'Completed',\n",
       " 'SecondaryStatus': 'Completed',\n",
       " 'HyperParameters': {'epochs': '5',\n",
       "  'feature_dim': '682754',\n",
       "  'mini_batch_size': '2000',\n",
       "  'num_factors': '64',\n",
       "  'predictor_type': 'regressor'},\n",
       " 'AlgorithmSpecification': {'TrainingImage': '991648021394.dkr.ecr.ap-south-1.amazonaws.com/factorization-machines:1',\n",
       "  'TrainingInputMode': 'File',\n",
       "  'MetricDefinitions': [{'Name': 'train:rmse:epoch',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, train rmse <loss>=(\\\\S+)'},\n",
       "   {'Name': 'train:progress',\n",
       "    'Regex': '#progress_metric: host=\\\\S+, completed (\\\\S+) %'},\n",
       "   {'Name': 'test:binary_f_beta',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, test binary_f_1.000 <score>=(\\\\S+)'},\n",
       "   {'Name': 'test:mse',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, test mse <loss>=(\\\\S+)'},\n",
       "   {'Name': 'train:binary_classification_accuracy:batch',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, batch=\\\\S+ train binary_classification_accuracy <score>=(\\\\S+)'},\n",
       "   {'Name': 'train:mse:batch',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, batch=\\\\S+ train mse <loss>=(\\\\S+)'},\n",
       "   {'Name': 'train:absolute_loss:epoch',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, train absolute_loss <loss>=(\\\\S+)'},\n",
       "   {'Name': 'test:absolute_loss',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, test absolute_loss <loss>=(\\\\S+)'},\n",
       "   {'Name': 'train:mse',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, train mse <loss>=(\\\\S+)'},\n",
       "   {'Name': 'train:binary_f_beta:batch',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, batch=\\\\S+ train binary_f_1.000 <score>=(\\\\S+)'},\n",
       "   {'Name': 'train:binary_classification_cross_entropy:epoch',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, train binary_classification_cross_entropy <loss>=(\\\\S+)'},\n",
       "   {'Name': 'train:binary_f_beta',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, train binary_f_1.000 <score>=(\\\\S+)'},\n",
       "   {'Name': 'train:mse:epoch',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, train mse <loss>=(\\\\S+)'},\n",
       "   {'Name': 'train:rmse:batch',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, batch=\\\\S+ train rmse <loss>=(\\\\S+)'},\n",
       "   {'Name': 'train:binary_classification_cross_entropy',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, train binary_classification_cross_entropy <loss>=(\\\\S+)'},\n",
       "   {'Name': 'train:binary_f_beta:epoch',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, train binary_f_1.000 <score>=(\\\\S+)'},\n",
       "   {'Name': 'train:rmse',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, train rmse <loss>=(\\\\S+)'},\n",
       "   {'Name': 'test:rmse',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, test rmse <loss>=(\\\\S+)'},\n",
       "   {'Name': 'train:binary_classification_accuracy:epoch',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, train binary_classification_accuracy <score>=(\\\\S+)'},\n",
       "   {'Name': 'train:absolute_loss:batch',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, batch=\\\\S+ train absolute_loss <loss>=(\\\\S+)'},\n",
       "   {'Name': 'train:binary_classification_accuracy',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, train binary_classification_accuracy <score>=(\\\\S+)'},\n",
       "   {'Name': 'test:binary_classification_cross_entropy',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, test binary_classification_cross_entropy <loss>=(\\\\S+)'},\n",
       "   {'Name': 'train:absolute_loss',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, train absolute_loss <loss>=(\\\\S+)'},\n",
       "   {'Name': 'train:throughput',\n",
       "    'Regex': '#throughput_metric: host=\\\\S+, train throughput=(\\\\S+) records/second'},\n",
       "   {'Name': 'test:binary_classification_accuracy',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, test binary_classification_accuracy <score>=(\\\\S+)'},\n",
       "   {'Name': 'train:binary_classification_cross_entropy:batch',\n",
       "    'Regex': '#quality_metric: host=\\\\S+, epoch=\\\\S+, batch=\\\\S+ train binary_classification_cross_entropy <loss>=(\\\\S+)'}],\n",
       "  'EnableSageMakerMetricsTimeSeries': False},\n",
       " 'RoleArn': 'arn:aws:iam::659144925604:role/service-role/AmazonSageMaker-ExecutionRole-20220320T214029',\n",
       " 'InputDataConfig': [{'ChannelName': 'train',\n",
       "   'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "     'S3Uri': 's3://sagemaker-ap-south-1-659144925604/filteredfm/fm_train_filtered_sparse.recordio',\n",
       "     'S3DataDistributionType': 'FullyReplicated'}},\n",
       "   'CompressionType': 'None',\n",
       "   'RecordWrapperType': 'None'},\n",
       "  {'ChannelName': 'test',\n",
       "   'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "     'S3Uri': 's3://sagemaker-ap-south-1-659144925604/filteredfm/fm_test_filtered_sparse.recordio',\n",
       "     'S3DataDistributionType': 'FullyReplicated'}},\n",
       "   'CompressionType': 'None',\n",
       "   'RecordWrapperType': 'None'}],\n",
       " 'OutputDataConfig': {'KmsKeyId': '',\n",
       "  'S3OutputPath': 's3://sagemaker-ap-south-1-659144925604/filteredfm/output'},\n",
       " 'ResourceConfig': {'InstanceType': 'ml.m4.xlarge',\n",
       "  'InstanceCount': 1,\n",
       "  'VolumeSizeInGB': 30},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 3600},\n",
       " 'CreationTime': datetime.datetime(2022, 4, 5, 11, 41, 24, 673000, tzinfo=tzlocal()),\n",
       " 'TrainingStartTime': datetime.datetime(2022, 4, 5, 11, 43, 4, 28000, tzinfo=tzlocal()),\n",
       " 'TrainingEndTime': datetime.datetime(2022, 4, 5, 11, 51, 23, 137000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2022, 4, 5, 11, 51, 30, 125000, tzinfo=tzlocal()),\n",
       " 'SecondaryStatusTransitions': [{'Status': 'Starting',\n",
       "   'StartTime': datetime.datetime(2022, 4, 5, 11, 41, 24, 673000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2022, 4, 5, 11, 43, 4, 28000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Preparing the instances for training'},\n",
       "  {'Status': 'Downloading',\n",
       "   'StartTime': datetime.datetime(2022, 4, 5, 11, 43, 4, 28000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2022, 4, 5, 11, 43, 34, 492000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Downloading input data'},\n",
       "  {'Status': 'Training',\n",
       "   'StartTime': datetime.datetime(2022, 4, 5, 11, 43, 34, 492000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2022, 4, 5, 11, 50, 47, 305000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Training image download completed. Training in progress.'},\n",
       "  {'Status': 'Uploading',\n",
       "   'StartTime': datetime.datetime(2022, 4, 5, 11, 50, 47, 305000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2022, 4, 5, 11, 51, 23, 137000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Uploading generated training model'},\n",
       "  {'Status': 'Completed',\n",
       "   'StartTime': datetime.datetime(2022, 4, 5, 11, 51, 23, 137000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2022, 4, 5, 11, 51, 23, 137000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Training job completed'}],\n",
       " 'FinalMetricDataList': [{'MetricName': 'train:rmse:epoch',\n",
       "   'Value': 0.39308828115463257,\n",
       "   'Timestamp': datetime.datetime(2022, 4, 5, 11, 49, 47, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'train:progress',\n",
       "   'Value': 100.0,\n",
       "   'Timestamp': datetime.datetime(2022, 4, 5, 11, 49, 47, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'test:mse',\n",
       "   'Value': 0.18291665613651276,\n",
       "   'Timestamp': datetime.datetime(2022, 4, 5, 11, 50, 41, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'train:mse:batch',\n",
       "   'Value': 0.15521106123924255,\n",
       "   'Timestamp': datetime.datetime(2022, 4, 5, 11, 49, 31, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'train:absolute_loss:epoch',\n",
       "   'Value': 0.24981370568275452,\n",
       "   'Timestamp': datetime.datetime(2022, 4, 5, 11, 49, 47, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'test:absolute_loss',\n",
       "   'Value': 0.28177592158317566,\n",
       "   'Timestamp': datetime.datetime(2022, 4, 5, 11, 50, 41, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'train:mse',\n",
       "   'Value': 0.15451839566230774,\n",
       "   'Timestamp': datetime.datetime(2022, 4, 5, 11, 49, 47, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'train:mse:epoch',\n",
       "   'Value': 0.15451839566230774,\n",
       "   'Timestamp': datetime.datetime(2022, 4, 5, 11, 49, 47, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'train:rmse:batch',\n",
       "   'Value': 0.3939683437347412,\n",
       "   'Timestamp': datetime.datetime(2022, 4, 5, 11, 49, 31, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'train:rmse',\n",
       "   'Value': 0.39308828115463257,\n",
       "   'Timestamp': datetime.datetime(2022, 4, 5, 11, 49, 47, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'test:rmse',\n",
       "   'Value': 0.42768755555152893,\n",
       "   'Timestamp': datetime.datetime(2022, 4, 5, 11, 50, 41, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'train:absolute_loss:batch',\n",
       "   'Value': 0.25063556432724,\n",
       "   'Timestamp': datetime.datetime(2022, 4, 5, 11, 49, 31, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'train:absolute_loss',\n",
       "   'Value': 0.24981370568275452,\n",
       "   'Timestamp': datetime.datetime(2022, 4, 5, 11, 49, 47, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'train:throughput',\n",
       "   'Value': 42829.765625,\n",
       "   'Timestamp': datetime.datetime(2022, 4, 5, 11, 49, 47, tzinfo=tzlocal())}],\n",
       " 'EnableNetworkIsolation': False,\n",
       " 'EnableInterContainerTrafficEncryption': False,\n",
       " 'EnableManagedSpotTraining': False,\n",
       " 'TrainingTimeInSeconds': 499,\n",
       " 'BillableTimeInSeconds': 499,\n",
       " 'ProfilerConfig': {'S3OutputPath': 's3://sagemaker-ap-south-1-659144925604/filteredfm/output',\n",
       "  'ProfilingIntervalInMilliseconds': 500},\n",
       " 'ProfilerRuleConfigurations': [{'RuleConfigurationName': 'ProfilerReport-1649158884',\n",
       "   'RuleEvaluatorImage': '904829902805.dkr.ecr.ap-south-1.amazonaws.com/sagemaker-debugger-rules:latest',\n",
       "   'VolumeSizeInGB': 0,\n",
       "   'RuleParameters': {'rule_to_invoke': 'ProfilerReport'}}],\n",
       " 'ProfilerRuleEvaluationStatuses': [{'RuleConfigurationName': 'ProfilerReport-1649158884',\n",
       "   'RuleEvaluationJobArn': 'arn:aws:sagemaker:ap-south-1:659144925604:processing-job/fm-job-recommender-v1-2022-profilerreport-1649158884-7a47dd89',\n",
       "   'RuleEvaluationStatus': 'NoIssuesFound',\n",
       "   'LastModifiedTime': datetime.datetime(2022, 4, 5, 11, 51, 30, 119000, tzinfo=tzlocal())}],\n",
       " 'ProfilingStatus': 'Enabled',\n",
       " 'ResponseMetadata': {'RequestId': '85ca6788-7cc3-4841-848f-dae42c579da3',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '85ca6788-7cc3-4841-848f-dae42c579da3',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '8159',\n",
       "   'date': 'Tue, 05 Apr 2022 11:51:42 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_name = estimator.latest_training_job.job_name\n",
    "\n",
    "sagemaker_boto_client = boto3.Session(region_name=region).client(\"sagemaker\")\n",
    "training_job_info = sagemaker_boto_client.describe_training_job(TrainingJobName = job_name)\n",
    "training_job_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the serializer and desrializer for prediction request and parsing response. this will be used by inference endpoint for request and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "import json\n",
    "\n",
    "class fm_json_serializer(JSONSerializer):\n",
    "    def serialize(self, data):\n",
    "        js = {\"instances\": []}\n",
    "        for row in data:\n",
    "            js[\"instances\"].append({\"features\": row.tolist()})\n",
    "        return json.dumps(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count = 1,\n",
    "                             instance_type = \"ml.m5.xlarge\",\n",
    "                             endpoint_name = job_name,\n",
    "                             serializer = fm_json_serializer(),\n",
    "                             deserializer = JSONDeserializer(),\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id\n",
       "13592     166\n",
       "20817     127\n",
       "59752     119\n",
       "51858     109\n",
       "121825    104\n",
       "175000     95\n",
       "436100     95\n",
       "372271     94\n",
       "23465      92\n",
       "113247     91\n",
       "117899     89\n",
       "89926      88\n",
       "296306     88\n",
       "340784     87\n",
       "3130       87\n",
       "335884     86\n",
       "23691      85\n",
       "319709     85\n",
       "17148      85\n",
       "15002      84\n",
       "2468       84\n",
       "419726     83\n",
       "9178       83\n",
       "21563      82\n",
       "8975       82\n",
       "940        81\n",
       "97838      81\n",
       "22849      80\n",
       "167657     80\n",
       "48896      80\n",
       "Name: article_id, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rank.groupby(\"customer_id\").count()[\"article_id\"].sort_values(ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a sample data for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77654</th>\n",
       "      <td>13592</td>\n",
       "      <td>253448003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77655</th>\n",
       "      <td>13592</td>\n",
       "      <td>456163064</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77656</th>\n",
       "      <td>13592</td>\n",
       "      <td>456163069</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77657</th>\n",
       "      <td>13592</td>\n",
       "      <td>506098007</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77658</th>\n",
       "      <td>13592</td>\n",
       "      <td>547780001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931440</th>\n",
       "      <td>13592</td>\n",
       "      <td>913030001</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931441</th>\n",
       "      <td>13592</td>\n",
       "      <td>915526002</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931442</th>\n",
       "      <td>13592</td>\n",
       "      <td>933706001</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3338200</th>\n",
       "      <td>13592</td>\n",
       "      <td>777099001</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3338201</th>\n",
       "      <td>13592</td>\n",
       "      <td>827411001</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         customer_id  article_id  rating\n",
       "77654          13592   253448003       1\n",
       "77655          13592   456163064       1\n",
       "77656          13592   456163069       1\n",
       "77657          13592   506098007       1\n",
       "77658          13592   547780001       1\n",
       "...              ...         ...     ...\n",
       "2931440        13592   913030001       2\n",
       "2931441        13592   915526002       2\n",
       "2931442        13592   933706001       2\n",
       "3338200        13592   777099001       4\n",
       "3338201        13592   827411001       4\n",
       "\n",
       "[166 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select one of the top customers from above.\n",
    "# This customer have records for both product categories.\n",
    "sample_customer = 13592 # 42799904, 50623001, 16528195, 35178127, 18167714\n",
    "\n",
    "# The existing product ratings given by the selected customer.\n",
    "df[df[\"customer_id\"] == sample_customer]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>unique_customers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>706016001</td>\n",
       "      <td>13238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>759871002</td>\n",
       "      <td>10717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>610776002</td>\n",
       "      <td>10699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>720125001</td>\n",
       "      <td>10626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>372860001</td>\n",
       "      <td>10173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>750330003</td>\n",
       "      <td>916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>570004009</td>\n",
       "      <td>907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>189634001</td>\n",
       "      <td>882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>717464002</td>\n",
       "      <td>853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>565379002</td>\n",
       "      <td>841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1684 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id  unique_customers\n",
       "0      706016001             13238\n",
       "1      759871002             10717\n",
       "2      610776002             10699\n",
       "3      720125001             10626\n",
       "4      372860001             10173\n",
       "...          ...               ...\n",
       "1679   750330003               916\n",
       "1680   570004009               907\n",
       "1681   189634001               882\n",
       "1682   717464002               853\n",
       "1683   565379002               841\n",
       "\n",
       "[1684 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trending = df_rank.copy()\n",
    "trending = (trending.groupby([\"article_id\"])\n",
    "            .nunique()[\"customer_id\"]\n",
    "            .sort_values(ascending=False)\n",
    "            .reset_index()            \n",
    "           )            \n",
    "trending = trending.rename(columns={'customer_id': 'unique_customers'})\n",
    "trending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample=df_rank.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_sample['article_id']=706016001\n",
    "df_sample['customer_id']=sample_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>product_group_name</th>\n",
       "      <th>detail_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13592</td>\n",
       "      <td>706016001</td>\n",
       "      <td>1</td>\n",
       "      <td>Garment Upper body</td>\n",
       "      <td>Fitted, long-sleeved, polo-neck top in soft jersey.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  customer_id  article_id  rating  product_group_name  \\\n",
       "0      0        13592   706016001       1  Garment Upper body   \n",
       "\n",
       "                                           detail_desc  \n",
       "0  Fitted, long-sleeved, polo-neck top in soft jersey.  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the sparse matrix using the One hot encoding and tfidf for the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x682272 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ohe = OneHotEncoder(handle_unknown = \"ignore\")\n",
    "# ohe_cols = [\"customer_id\", \"article_id\"]\n",
    "# ohe.fit(df[ohe_cols])\n",
    "ohe_features = ohe.transform(df_sample[ohe_cols])\n",
    "ohe_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x482 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_sample = vectorizer.transform(df_sample[\"detail_desc\"])\n",
    "tfidf_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trending = hstack([ohe_features,tfidf_sample], format=\"csr\", dtype=\"float32\")\n",
    "X_trending.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the prediction for the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'score': 1.2170072793960571}]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = predictor.predict(X_trending.toarray())\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have deployed recommendation model based on Sagemaker inbuilt Factorization machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-south-1:394103062818:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
